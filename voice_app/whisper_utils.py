# voice_app/whisper_utils.py

import time
import os
import gc
import json
import re
import math
import tempfile
import subprocess
import wave
import audioop
import logging
import threading
import importlib.util
from typing import List, Tuple, Optional, Dict, Any

logger = logging.getLogger(__name__)

# whisperxëŠ” ì„ íƒì  ì˜ì¡´ì„± (import ì‹œì ì— ë¡œë”©/ë¡œê·¸ë¥¼ ìœ ë°œí•˜ì§€ ì•Šë„ë¡ ìŠ¤í™ë§Œ ì²´í¬)
WHISPERX_AVAILABLE = importlib.util.find_spec("whisperx") is not None

# ğŸ’¡ ëª¨ë¸ì€ "ì „ì‚¬í•  ë•Œë§Œ" 1íšŒ ë¡œë”© (lazy-load)
WHISPER_MODEL_NAME = "large-v3"
FORCED_LANGUAGE = "ko"
# NOTE: initial_promptëŠ” ì¢…ì¢… ì¶œë ¥ìœ¼ë¡œ "ìœ ì¶œ"ë  ìˆ˜ ìˆì–´, ìœ ì¶œë¼ë„ ëœ ì–´ìƒ‰í•˜ê³ 
# í›„ì²˜ë¦¬ë¡œ ì œê±°í•˜ê¸° ì‰¬ìš´ ì§§ì€ ì§€ì‹œë¬¸ìœ¼ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
INITIAL_PROMPT_KO = "í•œêµ­ì–´ë¡œë§Œ ì „ì‚¬í•˜ì„¸ìš”. ìŒì„±ì— ì—†ëŠ” ë¬¸ì¥ì€ ì“°ì§€ ë§ˆì„¸ìš”."

_WHISPER_MODEL = None
_WHISPER_MODEL_LOCK = threading.Lock()


def get_whisper_model():
    """Whisper ëª¨ë¸ì„ lazy loadingìœ¼ë¡œ ê°€ì ¸ì˜´.

    - import ì‹œì ì— ëª¨ë¸ì„ ë¡œë”©í•˜ì§€ ì•ŠìŒ
    - í”„ë¡œì„¸ìŠ¤ ë‚´ì—ì„œ ìµœì´ˆ 1íšŒë§Œ ë¡œë”©
    """
    global _WHISPER_MODEL
    if _WHISPER_MODEL is not None:
        return _WHISPER_MODEL

    with _WHISPER_MODEL_LOCK:
        if _WHISPER_MODEL is not None:
            return _WHISPER_MODEL
        try:
            import whisper  # heavy import
        except Exception:
            logger.exception("[Whisper] Failed to import whisper package")
            return None

        try:
            logger.info("[Whisper] Loading model (%s)...", WHISPER_MODEL_NAME)
            _WHISPER_MODEL = whisper.load_model(WHISPER_MODEL_NAME)
            logger.info("[Whisper] Model loaded (%s)", WHISPER_MODEL_NAME)
            return _WHISPER_MODEL
        except Exception:
            logger.exception("[Whisper] Failed to load model (%s)", WHISPER_MODEL_NAME)
            _WHISPER_MODEL = None
            return None


def _koreanize_common_english_tokens(text: str) -> str:
    """Best-effort normalization to keep outputs in Hangul when short English interjections appear.

    This does not attempt full translation; it only converts a small set of common tokens.
    """
    if not text:
        return text

    out = str(text)

    # Replace common short interjections (case-insensitive, whole-word)
    replacements = [
        # ìŒì—­(ë“¤ë¦¬ëŠ” ëŒ€ë¡œ) ìš°ì„ : ë²ˆì—­(meaning) ê¸ˆì§€
        (re.compile(r"\bgood\b", re.IGNORECASE), "êµ¿"),
        (re.compile(r"\bokay\b", re.IGNORECASE), "ì˜¤ì¼€ì´"),
        (re.compile(r"\bok\b", re.IGNORECASE), "ì˜¤ì¼€ì´"),
    ]
    for pattern, replacement in replacements:
        out = pattern.sub(replacement, out)

    return out


def _scrub_prompt_leakage(text: str) -> str:
    """Remove known prompt/instruction phrases when they leak into model outputs.

    Whisper/WhisperXëŠ” initial_prompt ë¬¸êµ¬ë¥¼ ê·¸ëŒ€ë¡œ ì¶œë ¥ì— ì„ëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤.
    ì‹¤ì œ ìŒì„±ì— ì—†ëŠ” 'ì§€ì‹œë¬¸'ì´ ê²°ê³¼ì— ë¼ì–´ë“¤ë©´ í‘œì‹œ/API ë‹¨ê³„ì—ì„œ ì œê±°í•©ë‹ˆë‹¤.
    """
    if not text:
        return text

    def _normalize(s: str) -> str:
        s = (s or "").strip()
        s = re.sub(r"\s+", " ", s)
        # í”í•œ êµ¬ë‘ì /ëŒ€ì‹œë§Œ ì œê±° (ì˜ë¯¸ ë¬¸ì¥ í›¼ì† ìµœì†Œí™”)
        s = s.strip(" \t\r\n\"'`â€œâ€â€˜â€™.,!?;:Â·â€¦-â€“â€”")
        return s

    # ê³¼ê±°/í˜„ì¬ í”„ë¡¬í”„íŠ¸ ìœ ì¶œë¡œ ìì£¼ ë³´ì´ëŠ” ë¬¸êµ¬ë“¤
    banned_exact = {
        _normalize("í•œêµ­ì–´ ìŒì„±ì˜ ì „ì‚¬"),
        _normalize("ë‹¤ìŒì€ í•œêµ­ì–´ ìŒì„±ì˜ ì „ì‚¬ì…ë‹ˆë‹¤"),
        _normalize("ë‹¤ìŒì€ í•œêµ­ì–´ ìŒì„±ì˜ ì „ì‚¬ì…ë‹ˆë‹¤."),
        _normalize("ê°€ëŠ¥í•œ í•œ ì •í™•íˆ, ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì „ì‚¬í•˜ì„¸ìš”"),
        _normalize("í•œêµ­ì–´ë¡œë§Œ ì „ì‚¬í•˜ì„¸ìš”"),
        _normalize("ìŒì„±ì— ì—†ëŠ” ë¬¸ì¥ì€ ì“°ì§€ ë§ˆì„¸ìš”"),
        _normalize("í•œêµ­ì–´ ì™¸ ì–¸ì–´ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”"),
        _normalize("ì™¸êµ­ì–´ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”"),
        _normalize("ì¼ë³¸ì–´/ì˜ì–´/ì¤‘êµ­ì–´ ë“± ì™¸êµ­ì–´ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”"),
        _normalize(INITIAL_PROMPT_KO),
    }

    banned_substrings = [
        "í•œêµ­ì–´ ìŒì„±ì˜ ì „ì‚¬",
        "ë‹¤ìŒì€ í•œêµ­ì–´ ìŒì„±ì˜ ì „ì‚¬",
        "í•œêµ­ì–´ ì™¸ ì–¸ì–´ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”",
        "ì™¸êµ­ì–´ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”",
    ]

    # ë¼ì¸ ë(ë˜ëŠ” ì‹œì‘)ì— ë¶™ëŠ” ì§€ì‹œë¬¸ ë³€í˜• ì œê±°ìš©
    instruction_tail_re = re.compile(
        r"\s*(?:"
        r"í•œêµ­ì–´\s*ì™¸\s*ì–¸ì–´ë¡œ\s*ì¶œë ¥í•˜ì§€\s*ë§ˆì„¸ìš”|"
        r"ì™¸êµ­ì–´ë¡œ\s*ì¶œë ¥í•˜ì§€\s*ë§ˆì„¸ìš”|"
        r"ì¼ë³¸ì–´/ì˜ì–´/ì¤‘êµ­ì–´\s*ë“±\s*ì™¸êµ­ì–´ë¡œ\s*ì¶œë ¥í•˜ì§€\s*ë§ˆì„¸ìš”|"
        r"í•œêµ­ì–´ë¡œë§Œ\s*ì „ì‚¬í•˜ì„¸ìš”|"
        r"ìŒì„±ì—\s*ì—†ëŠ”\s*ë¬¸ì¥ì€\s*ì“°ì§€\s*ë§ˆì„¸ìš”|"
        r"ê°€ëŠ¥í•œ\s*í•œ\s*ì •í™•íˆ,\s*ë°˜ë“œì‹œ\s*í•œêµ­ì–´ë¡œë§Œ\s*ì „ì‚¬í•˜ì„¸ìš”|"
        r"ë‹¤ìŒì€\s*í•œêµ­ì–´\s*ìŒì„±ì˜\s*ì „ì‚¬ì…ë‹ˆë‹¤"
        r")\s*[\"'`â€œâ€â€˜â€™.,!?;:Â·â€¦\-â€“â€”]*\s*$"
    )
    instruction_head_re = re.compile(
        r"^\s*(?:"
        r"í•œêµ­ì–´\s*ì™¸\s*ì–¸ì–´ë¡œ\s*ì¶œë ¥í•˜ì§€\s*ë§ˆì„¸ìš”|"
        r"ì™¸êµ­ì–´ë¡œ\s*ì¶œë ¥í•˜ì§€\s*ë§ˆì„¸ìš”|"
        r"ì¼ë³¸ì–´/ì˜ì–´/ì¤‘êµ­ì–´\s*ë“±\s*ì™¸êµ­ì–´ë¡œ\s*ì¶œë ¥í•˜ì§€\s*ë§ˆì„¸ìš”|"
        r"í•œêµ­ì–´ë¡œë§Œ\s*ì „ì‚¬í•˜ì„¸ìš”|"
        r"ìŒì„±ì—\s*ì—†ëŠ”\s*ë¬¸ì¥ì€\s*ì“°ì§€\s*ë§ˆì„¸ìš”|"
        r"ê°€ëŠ¥í•œ\s*í•œ\s*ì •í™•íˆ,\s*ë°˜ë“œì‹œ\s*í•œêµ­ì–´ë¡œë§Œ\s*ì „ì‚¬í•˜ì„¸ìš”|"
        r"ë‹¤ìŒì€\s*í•œêµ­ì–´\s*ìŒì„±ì˜\s*ì „ì‚¬ì…ë‹ˆë‹¤"
        r")\s*[\"'`â€œâ€â€˜â€™.,!?;:Â·â€¦\-â€“â€”]*\s*"
    )

    cleaned_lines: List[str] = []
    for raw_line in str(text).splitlines():
        line = raw_line

        # ë¼ì¸ ì•/ë’¤ì— ë¶™ëŠ” ì§€ì‹œë¬¸ ì œê±° (ìŒì„±ì— ì‹¤ì œë¡œ ì—†ì„ í™•ë¥ ì´ ë§¤ìš° ë†’ìŒ)
        line = instruction_head_re.sub("", line)
        line = instruction_tail_re.sub("", line)
        # ì§€ì‹œë¬¸ ì œê±° í›„ ë‚¨ëŠ” ì„ í–‰ êµ¬ë‘ì /ê³µë°± ì •ë¦¬
        line = line.lstrip(" \t\r\n\"'`â€œâ€â€˜â€™.,!?;:Â·â€¦-â€“â€”")

        # ë¼ì¸ ì „ì²´ê°€ ì§€ì‹œë¬¸ì´ë©´ ì œê±°
        if _normalize(line) in banned_exact:
            continue

        # ë¼ì¸ ì¤‘ê°„ì— ë¼ì–´ë“  ëŒ€í‘œ ë¬¸êµ¬ëŠ” ì œê±°
        for sub in banned_substrings:
            if sub in line:
                line = line.replace(sub, "")

        # ì œê±° í›„ ë¹„ë©´ drop
        if not _normalize(line):
            continue

        cleaned_lines.append(line.strip())

    out = "\n".join(cleaned_lines).strip()
    # ë‹¤ì¤‘ ê³µë°± ì •ë¦¬
    out = re.sub(r"\s{2,}", " ", out)
    return out

model = None

# WhisperX ëª¨ë¸ ì „ì—­ ë³€ìˆ˜ (lazy loading) - whisperx ì‚¬ìš© ì‹œì—ë§Œ í•„ìš”
whisperx_model = None
whisperx_model_a = None
whisperx_metadata = None
diarize_model = None

def get_whisperx_model():
    """WhisperX ëª¨ë¸ì„ lazy loadingìœ¼ë¡œ ê°€ì ¸ì˜´"""
    global whisperx_model, whisperx_model_a, whisperx_metadata, diarize_model
    
    if not WHISPERX_AVAILABLE:
        logger.debug("[WhisperX] Not available (package not installed)")
        return None, None, None

    # heavy imports only when actually needed
    try:
        import torch
        import whisperx
    except Exception:
        logger.exception("[WhisperX] Failed to import whisperx/torch")
        return None, None, None
    
    if whisperx_model is None:
        try:
            device = "cuda" if torch.cuda.is_available() else "cpu"
            batch_size = 8 if device == "cuda" else 4  # batch size ì¤„ì„
            compute_type = "float16" if device == "cuda" else "int8"
            
            logger.info("[WhisperX] Loading model (large-v3) on %s...", device)
            # large-v3 ëª¨ë¸ ì‚¬ìš©
            whisperx_model = whisperx.load_model("large-v3", device, compute_type=compute_type, language="ko")
            
            # alignment model (í•œêµ­ì–´ ì§€ì›)
            whisperx_model_a, whisperx_metadata = whisperx.load_align_model(language_code="ko", device=device)
            
            logger.info("[WhisperX] Models loaded (large-v3)")
        except Exception as e:
            logger.exception("[WhisperX] Failed to load models")
            return None, None, None
    
    return whisperx_model, whisperx_model_a, whisperx_metadata


def transcribe_audio(audio_path):
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ Whisperë¡œ ì „ì‚¬í•˜ê³  ê²°ê³¼ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜.
    ì‹¤íŒ¨ ì‹œ None ë°˜í™˜.

    Args:
        audio_path (str): íŒŒì¼ ê²½ë¡œ

    Returns:
        str or None: ì „ì‚¬ ê²°ê³¼ ë˜ëŠ” None
    """
    # Load model only when we really transcribe
    m = get_whisper_model()
    if not m:
        logger.error("[Whisper] Model not loaded")
        return None

    if not os.path.exists(audio_path):
        logger.error("[Whisper] File does not exist: %s", audio_path)
        return None

    def _looks_korean(text: str) -> bool:
        if not text:
            return False
        hangul_count = len(re.findall(r"[\uac00-\ud7a3]", text))
        # ê³µë°±/êµ¬ë‘ì  ì œì™¸í•œ ëŒ€ëµì ì¸ ê¸€ììˆ˜ë¡œ ë¹„ìœ¨ ê³„ì‚°
        core = re.sub(r"\s+", "", text)
        core = re.sub(r"[^0-9A-Za-z\uac00-\ud7a3]", "", core)
        if not core:
            return False
        return hangul_count > 0 and (hangul_count / max(len(core), 1)) >= 0.10

    def _contains_japanese(text: str) -> bool:
        if not text:
            return False
        # Hiragana/Katakana/CJK(í•œì í¬í•¨) ë²”ìœ„ë¥¼ ë„“ê²Œ ì²´í¬
        return bool(re.search(r"[\u3040-\u30ff\u4e00-\u9fff]", text))

    try:
        start = time.time()

        try:
            import torch
            fp16 = torch.cuda.is_available()
        except Exception:
            fp16 = False

        options = {
            'fp16': fp16,
            'temperature': 0.0,
            'language': FORCED_LANGUAGE,
            'task': 'transcribe',
            'initial_prompt': INITIAL_PROMPT_KO,
            'beam_size': 5,
            'best_of': 5,
            'verbose': False,
        }

        try:
            result = m.transcribe(audio_path, **options)
        except TypeError as e:
            # whisper ë²„ì „ì— ë”°ë¼ ì˜µì…˜ì´ ë¯¸ì§€ì›ì¼ ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ ì¬ì‹œë„
            msg = str(e)
            for key in ('task', 'initial_prompt', 'beam_size', 'best_of'):
                if key in options and f"'{key}'" in msg:
                    options.pop(key, None)
            result = m.transcribe(audio_path, **options)

        text = (result.get('text') or '').strip()

        # ê²°ê³¼ê°€ í•œêµ­ì–´ë¡œ ë³´ì´ì§€ ì•Šìœ¼ë©´(í•œê¸€ì´ ê±°ì˜ ì—†ìœ¼ë©´) ë” ê°•í•œ í”„ë¡¬í”„íŠ¸ë¡œ 1íšŒ ì¬ì‹œë„
        if text and not _looks_korean(text):
            retry_options = dict(options)
            retry_options['temperature'] = 0.0
            retry_options['language'] = FORCED_LANGUAGE
            retry_options['initial_prompt'] = INITIAL_PROMPT_KO + " í•œêµ­ì–´ ì™¸ ì–¸ì–´ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”."
            try:
                result = m.transcribe(audio_path, **retry_options)
                text = (result.get('text') or '').strip()
            except Exception:
                # ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ìµœì´ˆ ê²°ê³¼ ìœ ì§€
                pass

        # ì¼ë³¸ì–´/í•œì ë“±ì´ ì„ì´ê³  í•œêµ­ì–´ë¡œ ë³´ì´ì§€ ì•Šìœ¼ë©´ 1íšŒ ì¶”ê°€ ì¬ì‹œë„
        if text and _contains_japanese(text) and not _looks_korean(text):
            retry2_options = dict(options)
            retry2_options['temperature'] = 0.0
            retry2_options['language'] = FORCED_LANGUAGE
            retry2_options['initial_prompt'] = (
                INITIAL_PROMPT_KO
                + " ì¼ë³¸ì–´/ì˜ì–´/ì¤‘êµ­ì–´ ë“± ì™¸êµ­ì–´ë¡œ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”. "
                + "ë°˜ë“œì‹œ í•œê¸€ë¡œë§Œ ì „ì‚¬í•˜ì„¸ìš”."
            )
            try:
                result = m.transcribe(audio_path, **retry2_options)
                text = (result.get('text') or '').strip()
            except Exception:
                pass
        # ìµœì¢…ì ìœ¼ë¡œ ì§§ì€ ì˜ì–´ í† í°ì„ í•œê¸€í™” (ì˜ˆ: "Good" -> "ì¢‹ì•„")
        text = _koreanize_common_english_tokens(text)
        text = _scrub_prompt_leakage(text)

        elapsed = time.time() - start
        logger.info("[Whisper] Transcription completed in %.2f seconds", elapsed)
        return text
    except Exception as e:
        logger.exception("[Whisper] Failed to transcribe %s", audio_path)
        return None


def _ffmpeg_convert_to_wav_16k_mono(input_path: str) -> str:
    """Convert any audio file into a temporary 16kHz mono WAV (PCM 16-bit)."""
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    tmp.close()
    out_path = tmp.name
    cmd = [
        "ffmpeg", "-y",
        "-i", input_path,
        "-ac", "1",
        "-ar", "16000",
        "-c:a", "pcm_s16le",
        out_path,
    ]
    try:
        subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
        return out_path
    except Exception as e:
        try:
            os.unlink(out_path)
        except Exception:
            pass
        raise RuntimeError(f"ffmpeg convert failed: {e}")


def _extract_wav_segment(input_path: str, start_s: float, end_s: float) -> str:
    """Extract a time range to a temporary WAV (16kHz mono) for stable ASR."""
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    tmp.close()
    out_path = tmp.name
    cmd = [
        "ffmpeg", "-y",
        "-i", input_path,
        "-ss", f"{start_s:.3f}",
        "-to", f"{end_s:.3f}",
        "-ac", "1",
        "-ar", "16000",
        "-c:a", "pcm_s16le",
        out_path,
    ]
    try:
        subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
        return out_path
    except Exception as e:
        try:
            os.unlink(out_path)
        except Exception:
            pass
        raise RuntimeError(f"ffmpeg segment extract failed: {e}")


def _merge_intervals(intervals: List[Tuple[float, float]], max_gap_s: float = 0.25) -> List[Tuple[float, float]]:
    if not intervals:
        return []
    intervals_sorted = sorted(intervals, key=lambda x: x[0])
    merged: List[Tuple[float, float]] = [intervals_sorted[0]]
    for start, end in intervals_sorted[1:]:
        prev_start, prev_end = merged[-1]
        if start <= prev_end + max_gap_s:
            merged[-1] = (prev_start, max(prev_end, end))
        else:
            merged.append((start, end))
    return merged


def _split_long_intervals(
    intervals: List[Tuple[float, float]],
    max_len_s: float = 20.0,
    overlap_s: float = 0.15,
) -> List[Tuple[float, float]]:
    if not intervals:
        return []
    out: List[Tuple[float, float]] = []
    for start, end in intervals:
        length = end - start
        if length <= max_len_s:
            out.append((start, end))
            continue
        t = start
        while t < end:
            chunk_end = min(t + max_len_s, end)
            out.append((t, chunk_end))
            if chunk_end >= end:
                break
            t = max(chunk_end - overlap_s, t + 0.01)
    return out


def vad_detect_speech_segments(
    audio_path: str,
    frame_ms: int = 30,
    threshold_dbfs: float = -35.0,
    min_speech_ms: int = 250,
    padding_ms: int = 200,
) -> List[Tuple[float, float]]:
    """Energy-based VAD (no extra deps): returns speech segments as (start_s, end_s).

    This improves mixed child/adult recordings by removing long silences and isolating short utterances.
    """
    wav_path = _ffmpeg_convert_to_wav_16k_mono(audio_path)
    try:
        with wave.open(wav_path, 'rb') as wf:
            sample_rate = wf.getframerate()
            sample_width = wf.getsampwidth()
            channels = wf.getnchannels()
            if sample_rate != 16000 or sample_width != 2 or channels != 1:
                raise RuntimeError("Unexpected WAV format after conversion")

            frame_size = int(sample_rate * (frame_ms / 1000.0))
            bytes_per_frame = frame_size * sample_width

            segments: List[Tuple[float, float]] = []
            in_speech = False
            speech_start = 0.0
            last_voiced_t = 0.0

            frame_index = 0
            while True:
                pcm = wf.readframes(frame_size)
                if not pcm:
                    break

                rms = audioop.rms(pcm, sample_width)
                # Convert RMS to dBFS-like value; clamp to avoid log(0)
                if rms <= 0:
                    dbfs = -120.0
                else:
                    dbfs = 20.0 * math.log10(rms / 32768.0)

                t = (frame_index * frame_size) / sample_rate
                voiced = dbfs >= threshold_dbfs

                if voiced:
                    last_voiced_t = t + (frame_ms / 1000.0)
                    if not in_speech:
                        in_speech = True
                        speech_start = max(0.0, t - (padding_ms / 1000.0))
                else:
                    if in_speech:
                        # if we've been non-voiced beyond padding, close segment
                        gap = t - last_voiced_t
                        if gap >= (padding_ms / 1000.0):
                            speech_end = min(last_voiced_t + (padding_ms / 1000.0), t)
                            if (speech_end - speech_start) * 1000.0 >= min_speech_ms:
                                segments.append((speech_start, speech_end))
                            in_speech = False

                frame_index += 1

            # flush tail
            if in_speech:
                speech_end = last_voiced_t + (padding_ms / 1000.0)
                if (speech_end - speech_start) * 1000.0 >= min_speech_ms:
                    segments.append((speech_start, speech_end))

        segments = _merge_intervals(segments, max_gap_s=0.25)
        return segments
    finally:
        try:
            os.unlink(wav_path)
        except Exception:
            pass


def transcribe_audio_vad_segmented(
    audio_path: str,
    max_segment_s: float = 20.0,
    vad_threshold_dbfs: float = -35.0,
) -> Optional[str]:
    """VAD ê¸°ë°˜ìœ¼ë¡œ êµ¬ê°„ì„ ë‚˜ëˆˆ ë’¤ êµ¬ê°„ë³„ ì „ì‚¬í•˜ì—¬ í•©ì¹©ë‹ˆë‹¤."""
    segments = vad_detect_speech_segments(audio_path, threshold_dbfs=vad_threshold_dbfs)
    if not segments:
        # fallback to whole-file transcription
        return transcribe_audio(audio_path)

    segments = _split_long_intervals(segments, max_len_s=max_segment_s, overlap_s=0.15)

    texts: List[str] = []
    for start_s, end_s in segments:
        # ë„ˆë¬´ ì§§ì€ ì¡°ê°ì€ ì „ì‚¬ ë…¸ì´ì¦ˆê°€ ë§ì•„ì„œ skip
        if (end_s - start_s) < 0.20:
            continue
        seg_path = None
        try:
            seg_path = _extract_wav_segment(audio_path, start_s, end_s)
            seg_text = transcribe_audio(seg_path)
            if seg_text:
                texts.append(seg_text.strip())
        except Exception as e:
            print(f"[VAD] Segment transcribe failed ({start_s:.2f}-{end_s:.2f}s): {e}")
        finally:
            if seg_path:
                try:
                    os.unlink(seg_path)
                except Exception:
                    pass

    merged_text = " ".join([t for t in texts if t]).strip()
    return merged_text if merged_text else transcribe_audio(audio_path)


def transcribe_audio_with_diarization(
    audio_path: str,
    min_speakers: int = 1,
    max_speakers: int = 2,
    vad_threshold_dbfs: float = -35.0,
    max_segment_s: float = 20.0,
    include_speaker_labels: bool = False,
) -> Dict[str, Any]:
    """Pyannote diarization í›„, í™”ì ë¼ë²¨ì„ ë¶™ì—¬ ì „ì‚¬ë¥¼ í•©ì¹©ë‹ˆë‹¤.

    ì‹¤íŒ¨í•˜ê±°ë‚˜ ì˜ì¡´ì„±ì´ ì—†ìœ¼ë©´ VAD-onlyë¡œ ì•ˆì „í•˜ê²Œ í´ë°±í•©ë‹ˆë‹¤.
    """
    try:
        from .diarization_utils import SpeakerDiarizer
    except Exception as e:
        text = transcribe_audio_vad_segmented(audio_path, max_segment_s=max_segment_s, vad_threshold_dbfs=vad_threshold_dbfs)
        return {
            'success': True,
            'mode': 'vad',
            'text': text or '',
            'diarization': None,
            'error': f"diarization unavailable: {e}",
        }

    diarizer = SpeakerDiarizer()
    diarization = diarizer.perform_diarization(
        audio_path,
        num_speakers=None,
        min_speakers=min_speakers,
        max_speakers=max_speakers,
    )
    if diarization.get('status') != 'completed' or not diarization.get('segments'):
        text = transcribe_audio_vad_segmented(audio_path, max_segment_s=max_segment_s, vad_threshold_dbfs=vad_threshold_dbfs)
        return {
            'success': True,
            'mode': 'vad',
            'text': text or '',
            'diarization': diarization,
            'error': diarization.get('error'),
        }

    diarization = diarizer.assign_speaker_labels(diarization)

    # Merge adjacent same-speaker segments to reduce ffmpeg calls
    merged_segments: List[Dict[str, Any]] = []
    for seg in diarization['segments']:
        if not merged_segments:
            merged_segments.append(seg.copy())
            continue
        prev = merged_segments[-1]
        if seg['speaker'] == prev['speaker'] and seg['start'] <= prev['end'] + 0.25:
            prev['end'] = max(prev['end'], seg['end'])
            prev['duration'] = float(prev['end'] - prev['start'])
        else:
            merged_segments.append(seg.copy())

    lines: List[str] = []
    for seg in merged_segments:
        speaker = seg.get('speaker', 'í™”ì')
        start_s = float(seg.get('start', 0.0))
        end_s = float(seg.get('end', start_s))
        if end_s - start_s < 0.20:
            continue
        # segmentë¥¼ ë˜ VADë¡œ ì˜ë¼ì„œ(ì§§ì€ ë‹¨ì–´/ë¹ ë¥¸ ë°œí™” ëŒ€ì‘) ì „ì‚¬ í›„ í•©ì¹˜ê¸°
        seg_path = None
        try:
            seg_path = _extract_wav_segment(audio_path, start_s, end_s)
            seg_text = transcribe_audio_vad_segmented(seg_path, max_segment_s=max_segment_s, vad_threshold_dbfs=vad_threshold_dbfs)
            seg_text = (seg_text or '').strip()
            if seg_text:
                if include_speaker_labels:
                    lines.append(f"[{speaker}] {seg_text}")
                else:
                    lines.append(seg_text)
        except Exception as e:
            print(f"[Diarization] Segment transcribe failed ({speaker} {start_s:.2f}-{end_s:.2f}s): {e}")
        finally:
            if seg_path:
                try:
                    os.unlink(seg_path)
                except Exception:
                    pass

    text = "\n".join(lines).strip()
    if not text:
        text = transcribe_audio_vad_segmented(audio_path, max_segment_s=max_segment_s, vad_threshold_dbfs=vad_threshold_dbfs) or ''

    # diarization ê²°ê³¼ê°€ í•œêµ­ì–´ë¡œ ë³´ì´ì§€ ì•Šìœ¼ë©´(íŠ¹íˆ ì¼ë³¸ì–´/í•œì ë“±) ì „ì²´ íŒŒì¼ ë‹¨ì¼ ì „ì‚¬ë¡œ í´ë°±
    # - ë¼ë²¨ì„ ì œê±°í•œ í…ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ íŒë³„í•˜ì—¬ [ì•„ë™] ê°™ì€ ë¼ë²¨ì´ íŒì •ì„ ë°©í•´í•˜ì§€ ì•Šê²Œ í•¨
    try:
        def _looks_korean_text(s: str) -> bool:
            if not s:
                return False
            hangul_count = len(re.findall(r"[\uac00-\ud7a3]", s))
            core = re.sub(r"\s+", "", s)
            core = re.sub(r"[^0-9A-Za-z\uac00-\ud7a3]", "", core)
            if not core:
                return False
            return hangul_count > 0 and (hangul_count / max(len(core), 1)) >= 0.10

        def _contains_japanese_text(s: str) -> bool:
            if not s:
                return False
            return bool(re.search(r"[\u3040-\u30ff\u4e00-\u9fff]", s))

        if text and (not _looks_korean_text(text)) and _contains_japanese_text(text):
            fallback = transcribe_audio(audio_path)
            if fallback and _looks_korean_text(fallback):
                text = fallback
    except Exception:
        # íŒë³„/í´ë°± ì‹¤íŒ¨ ì‹œ diarization ê²°ê³¼ ìœ ì§€
        pass

    return {
        'success': True,
        'mode': 'diarization',
        'text': text,
        'diarization': diarization,
        'error': None,
    }


def transcribe_audio_mixed_child_adult(
    audio_path: str,
    prefer_diarization: bool = True,
    **kwargs,
) -> Dict[str, Any]:
    """5ì‚´ ì´í•˜+ì„±ì¸ í˜¼í•© ë°œí™”ì— ìµœì í™”ëœ ì „ì‚¬.

    - diarization ê°€ëŠ¥í•˜ë©´ í™”ì ë¼ë²¨ í¬í•¨ í…ìŠ¤íŠ¸ ë°˜í™˜
    - ì‹¤íŒ¨/ë¯¸ì„¤ì¹˜ ì‹œ VAD-onlyë¡œ í´ë°±
    """
    if prefer_diarization:
        return transcribe_audio_with_diarization(audio_path, **kwargs)
    text = transcribe_audio_vad_segmented(audio_path, **kwargs)
    return {
        'success': True,
        'mode': 'vad',
        'text': text or '',
        'diarization': None,
        'error': None,
    }


def transcribe_and_align_whisperx(audio_path):
    """
    WhisperXë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì‚¬ ë° forced alignment ìˆ˜í–‰
    
    Args:
        audio_path (str): ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        dict: {
            'transcription': str,
            'segments': list,
            'word_segments': list,
            'success': bool,
            'error': str or None
        }
    """
    if not WHISPERX_AVAILABLE:
        return {
            'transcription': '',
            'segments': [],
            'word_segments': [],
            'success': False,
            'error': 'WhisperX is not installed. Please install it with: pip install whisperx'
        }
    
    torch = None
    whisperx = None
    try:
        if not os.path.exists(audio_path):
            return {
                'transcription': '',
                'segments': [],
                'word_segments': [],
                'success': False,
                'error': f'File not found: {audio_path}'
            }
        
        # heavy imports only when actually needed
        import torch as _torch
        import whisperx as _whisperx
        torch = _torch
        whisperx = _whisperx

        device = "cuda" if torch.cuda.is_available() else "cpu"
        batch_size = 8 if device == "cuda" else 4  # batch size ì¤„ì„
        compute_type = "float16" if device == "cuda" else "int8"
        
        logger.info("[WhisperX] Starting transcription+alignment for: %s", audio_path)
        start_time = time.time()
        
        # 1. ì˜¤ë””ì˜¤ ë¡œë“œ
        audio = whisperx.load_audio(audio_path)
        
        # 2. WhisperX ëª¨ë¸ ë¡œë“œ
        whisperx_model, model_a, metadata = get_whisperx_model()
        if whisperx_model is None:
            return {
                'transcription': '',
                'segments': [],
                'word_segments': [],
                'success': False,
                'error': 'Failed to load WhisperX models'
            }
        
        # 3. ì „ì‚¬ ìˆ˜í–‰ (í•œêµ­ì–´ë¡œ ê³ ì •)
        wx_options = {
            'batch_size': batch_size,
            'language': FORCED_LANGUAGE,
            'task': 'transcribe',
            'initial_prompt': INITIAL_PROMPT_KO,
        }
        try:
            result = whisperx_model.transcribe(audio, **wx_options)
        except TypeError as e:
            msg = str(e)
            for key in ('task', 'initial_prompt', 'language'):
                if key in wx_options and f"'{key}'" in msg:
                    wx_options.pop(key, None)
            result = whisperx_model.transcribe(audio, **wx_options)
        
        # ì „ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        transcription = ""
        if 'segments' in result:
            transcription = " ".join([seg['text'] for seg in result['segments']])

        transcription = _koreanize_common_english_tokens(transcription)
        transcription = _scrub_prompt_leakage(transcription)
        
        # 4. Forced alignment ìˆ˜í–‰
        result = whisperx.align(result["segments"], model_a, metadata, audio, device, return_char_alignments=False)
        
        # 5. ê²°ê³¼ ì •ë¦¬
        segments = []
        word_segments = []
        
        if 'segments' in result:
            for segment in result['segments']:
                seg_text = segment.get('text', '')
                seg_text = _koreanize_common_english_tokens(seg_text)
                seg_text = _scrub_prompt_leakage(seg_text)
                seg_data = {
                    'start': segment.get('start', 0),
                    'end': segment.get('end', 0),
                    'text': seg_text,
                    'id': segment.get('id', 0)
                }
                segments.append(seg_data)
                
                # ë‹¨ì–´ ë ˆë²¨ alignment
                if 'words' in segment:
                    for word in segment['words']:
                        word_data = {
                            'start': word.get('start', 0),
                            'end': word.get('end', 0),
                            'word': word.get('word', ''),
                            'score': word.get('score', 0.0),
                            'segment_id': segment.get('id', 0)
                        }
                        word_segments.append(word_data)
        
        elapsed = time.time() - start_time
        logger.info("[WhisperX] Completed in %.2f seconds", elapsed)
        
        return {
            'transcription': transcription.strip(),
            'segments': segments,
            'word_segments': word_segments,
            'success': True,
            'error': None
        }
        
    except Exception as e:
        logger.exception("[WhisperX] Failed to process %s", audio_path)
        return {
            'transcription': '',
            'segments': [],
            'word_segments': [],
            'success': False,
            'error': str(e)
        }
    finally:
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        try:
            if torch is not None and torch.cuda.is_available():
                torch.cuda.empty_cache()
        except Exception:
            pass
        gc.collect()


def format_alignment_for_frontend(alignment_data):
    """
    alignment ë°ì´í„°ë¥¼ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜
    
    Args:
        alignment_data (dict): WhisperX alignment ê²°ê³¼
        
    Returns:
        dict: í”„ë¡ íŠ¸ì—”ë“œìš© í¬ë§·ëœ ë°ì´í„°
    """
    if not alignment_data or not alignment_data.get('success'):
        return {
            'segments': [],
            'words': [],
            'transcription': '',
            'duration': 0
        }
    
    segments = alignment_data.get('segments', [])
    words = alignment_data.get('word_segments', [])
    
    # ì „ì²´ ê¸¸ì´ ê³„ì‚°
    duration = 0
    if segments:
        duration = max([seg.get('end', 0) for seg in segments])
    elif words:
        duration = max([word.get('end', 0) for word in words])
    
    return {
        'segments': segments,
        'words': words,
        'transcription': alignment_data.get('transcription', ''),
        'duration': duration
    }

